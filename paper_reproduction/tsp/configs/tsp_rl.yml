# Arguments that get passed to the constructor of your class instance
# as config={} keyword
environment:
    gym_id: "TSP1000-20-10000-v0"
    n_workers: 10

main:
    # MODELS:
    # -----
    # For discrete action space environments:
    #     PPO2, DQN, ACER, A2C, ACKTR
    # For continuous action space environments:
    #     PPO2, A2C

    # POLICIES:
    # ---------
    #     Defaults:
    #         CnnPolicy - CNN as described in 2014 Atari paper
    #         ActorCriticPolicy - simple MLP with two hidden layers of size 64 (old name MlpPolicy)

    # to understand the steps/time setup of training read:
    # https://stackoverflow.com/questions/56700948/understanding-the-total-timesteps-parameter-in-stable-baselines-models

    model_name: PPO
    policy: MlpPolicy
    n_steps: 2000000
    save_interval: 200000

models:
    PPO:
        n_steps: 256         # Batch size (n_steps * n_workers)
        batch_size: 64      # Number of minibatches for SGD/Adam updates
        n_epochs: 10        # Number of iterations for SGD/Adam
        gamma: 0.99          # Discount factor for future rewards
        gae_lambda: 0.95     # Generalized advantage estimation, for controlling variance/bias tradeoff (lam in PPO2)
        clip_range: 0.2       # Clip factor for PPO (the action probability distribution of the updated policy cannot differ from the old one by this fraction [measured by KL divergence])
        ent_coef: 0.0       # Entropy loss coefficient (higher values encourage more exploration)
        learning_rate: 0.0003 # LR
        vf_coef: 0.5         # The contribution of value function loss to the total loss of the network
        max_grad_norm: 0.5   # Max range of the gradient clipping
        verbose: 1          # the verbosity level: 0 no output, 1 info, 2 debug
        device: "cpu"


    DQN:
        gamma: 0.99
        learning_rate: 0.001
        buffer_size: 20000
        exploration_fraction: 0.1
        exploration_final_eps: 0.01
        train_freq: 1
        batch_size: 32
        learning_starts: 1000
        target_network_update_freq: 500
        prioritized_replay: false
        prioritized_replay_alpha: 0.2
        prioritized_replay_beta0: 0.4
        prioritized_replay_beta_iters: None
        prioritized_replay_eps: 0.000001
        param_noise: False
        verbose: 1
        full_tensorboard_log: False
        _init_setup_model: True
    A2C:
        learning_rate: 0.0007
        n_steps: 5
        gamma: 0.99
        gae_lambda: 1.0
        ent_coef: 0.0
        vf_coef: 0.5
        max_grad_norm: 0.5
        verbose: 1

    ACER:
        gamma: 0.99
        n_steps: 20
        num_procs: 1
        q_coef: 0.5
        ent_coef: 0.01
        max_grad_norm: 10
        learning_rate: 0.0007
        lr_schedule: linear
        rprop_alpha: 0.99
        rprop_epsilon: 0.0001
        buffer_size: 5000
        replay_ratio: 4
        replay_start: 1000
        correction_term: 10.0
        trust_region: true
        alpha: 0.99
        delta: 1
        verbose: 0
    ACKTR:
        gamma: 0.99
        nprocs: 1
        n_steps: 20
        ent_coef: 0.01
        vf_coef: 0.25
        vf_fisher_coef: 1.0
        learning_rate: 0.25
        max_grad_norm: 0.5
        kfac_clip: 0.001
        lr_schedule: linear
        verbose: 0
        async_eigen_decomp: False
        full_tensorboard_log: False



